{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d1f51ee88d34b8fa9b81baba9acf58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_419f590f7ed94fe48b25ac783c7fea3b",
              "IPY_MODEL_fcad869f61f64472bf2994f4476bfa82",
              "IPY_MODEL_d0a27fdc7421408e9fa4121557767d2c"
            ],
            "layout": "IPY_MODEL_f2d7aedb22f54be2b4e536f2c57ad9c5"
          }
        },
        "419f590f7ed94fe48b25ac783c7fea3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f80623d1dece41b6997c8a613d0b28f1",
            "placeholder": "​",
            "style": "IPY_MODEL_2d4a44272c7241b8a20e8f6ade9f7f0e",
            "value": "Loading weights: 100%"
          }
        },
        "fcad869f61f64472bf2994f4476bfa82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76b33aca5e29445bb6386e8d700be177",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fd705b53b7b48a991891c53715c7254",
            "value": 103
          }
        },
        "d0a27fdc7421408e9fa4121557767d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b74ac964a35a43b3ad16d535c69add5a",
            "placeholder": "​",
            "style": "IPY_MODEL_c77c3bbb4b684a699dc07489f156c27d",
            "value": " 103/103 [00:00&lt;00:00, 471.21it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "f2d7aedb22f54be2b4e536f2c57ad9c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f80623d1dece41b6997c8a613d0b28f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4a44272c7241b8a20e8f6ade9f7f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76b33aca5e29445bb6386e8d700be177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fd705b53b7b48a991891c53715c7254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b74ac964a35a43b3ad16d535c69add5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77c3bbb4b684a699dc07489f156c27d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "6d1f51ee88d34b8fa9b81baba9acf58b",
            "419f590f7ed94fe48b25ac783c7fea3b",
            "fcad869f61f64472bf2994f4476bfa82",
            "d0a27fdc7421408e9fa4121557767d2c",
            "f2d7aedb22f54be2b4e536f2c57ad9c5",
            "f80623d1dece41b6997c8a613d0b28f1",
            "2d4a44272c7241b8a20e8f6ade9f7f0e",
            "76b33aca5e29445bb6386e8d700be177",
            "7fd705b53b7b48a991891c53715c7254",
            "b74ac964a35a43b3ad16d535c69add5a",
            "c77c3bbb4b684a699dc07489f156c27d"
          ]
        },
        "id": "RNryN-t2n6B5",
        "outputId": "abd3be35-ab9b-4ebf-a85e-5500c89c6845"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d1f51ee88d34b8fa9b81baba9acf58b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding flat retrieval DB...\n",
            "\n",
            "===== BASELINE =====\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 180/180 [05:50<00:00,  1.95s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== FLAT RETRIEVAL =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180/180 [05:24<00:00,  1.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== HIERARCHICAL MEMORY =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180/180 [06:25<00:00,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ FINAL COMPARISON ================\n",
            "System                Acc     Latency    Tokens\n",
            "--------------------------------------------------\n",
            "Baseline             0.4722   1.95s   289.9\n",
            "Flat Retrieval       0.5500   1.76s   720.0\n",
            "Hierarchical         0.4667   2.02s   684.8\n",
            "===================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# GSM8K MASTER RESEARCH SCRIPT\n",
        "# Baseline vs Flat Retrieval vs Hierarchical Memory\n",
        "# ==========================================================\n",
        "\n",
        "!pip install datasets sentence-transformers faiss-cpu -q\n",
        "\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# ==========================================================\n",
        "# CONFIGURATION (TOKEN SAFE)\n",
        "# ==========================================================\n",
        "\n",
        "API_KEY = \"csk-hkc9kwef3nwvvdftd3pthdwpj4jfj2n4t2rd8yvc5ww558p6\"\n",
        "BASE_URL = \"https://api.cerebras.ai/v1\"\n",
        "MODEL_NAME = \"llama3.1-8b\"\n",
        "\n",
        "DATASET_SIZE = 180\n",
        "TRAIN_RETRIEVAL_SIZE = 250\n",
        "\n",
        "NUM_RUNS = 1\n",
        "NUM_SAMPLES = 1\n",
        "TEMPERATURE = 0.1\n",
        "MAX_TOKENS = 200\n",
        "\n",
        "TOP_K = 3\n",
        "TOP_K_EPISODIC = 2\n",
        "TOP_K_FAILURE = 1\n",
        "\n",
        "MAX_MEMORY_SIZE = 120\n",
        "\n",
        "# ==========================================================\n",
        "# CLIENT\n",
        "# ==========================================================\n",
        "\n",
        "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
        "\n",
        "# ==========================================================\n",
        "# PROMPT\n",
        "# ==========================================================\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are a careful mathematical reasoner.\n",
        "\n",
        "Solve step by step.\n",
        "\n",
        "Final answer must be on a new line:\n",
        "#### <integer>\n",
        "\"\"\"\n",
        "\n",
        "# ==========================================================\n",
        "# UTILS\n",
        "# ==========================================================\n",
        "\n",
        "def extract_answer(text):\n",
        "    if text is None:\n",
        "        return None\n",
        "    match = re.search(r\"####\\s*(-?\\d+\\.?\\d*)\", text.replace(\",\", \"\"))\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def normalize_answer(ans):\n",
        "    try:\n",
        "        return float(ans)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def embed_normalized(model, text):\n",
        "    emb = model.encode([text])[0]\n",
        "    emb = np.array([emb]).astype(\"float32\")\n",
        "    faiss.normalize_L2(emb)\n",
        "    return emb\n",
        "\n",
        "# ==========================================================\n",
        "# LOAD DATA\n",
        "# ==========================================================\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
        "dataset = dataset.select(range(DATASET_SIZE))\n",
        "\n",
        "train_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "train_dataset = train_dataset.select(range(TRAIN_RETRIEVAL_SIZE))\n",
        "\n",
        "# ==========================================================\n",
        "# EMBEDDING MODEL (LOAD ONCE)\n",
        "# ==========================================================\n",
        "\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "EMBED_DIM = 384\n",
        "\n",
        "# ==========================================================\n",
        "# BASELINE\n",
        "# ==========================================================\n",
        "\n",
        "def run_baseline():\n",
        "\n",
        "    correct = 0\n",
        "    latencies = []\n",
        "    token_usages = []\n",
        "\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "\n",
        "        question = dataset[i][\"question\"]\n",
        "        gt = normalize_answer(extract_answer(dataset[i][\"answer\"]))\n",
        "\n",
        "        answers = []\n",
        "        sample_lat = []\n",
        "        sample_tok = []\n",
        "\n",
        "        for _ in range(NUM_SAMPLES):\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                    {\"role\":\"user\",\"content\":question}\n",
        "                ],\n",
        "                temperature=TEMPERATURE,\n",
        "                max_tokens=MAX_TOKENS\n",
        "            )\n",
        "\n",
        "            sample_lat.append(time.time()-start)\n",
        "\n",
        "            output = response.choices[0].message.content\n",
        "            pred = normalize_answer(extract_answer(output))\n",
        "\n",
        "            if pred is not None:\n",
        "                answers.append(pred)\n",
        "\n",
        "            if response.usage:\n",
        "                sample_tok.append(response.usage.total_tokens)\n",
        "\n",
        "        if answers:\n",
        "            vote = Counter(answers)\n",
        "            final = vote.most_common(1)[0][0]\n",
        "            if final == gt:\n",
        "                correct += 1\n",
        "\n",
        "        latencies.append(np.mean(sample_lat))\n",
        "        token_usages.append(np.sum(sample_tok))\n",
        "\n",
        "    return correct/len(dataset), np.mean(latencies), np.mean(token_usages)\n",
        "\n",
        "# ==========================================================\n",
        "# FLAT RETRIEVAL\n",
        "# ==========================================================\n",
        "\n",
        "print(\"Embedding flat retrieval DB...\")\n",
        "train_questions = [train_dataset[i][\"question\"] for i in range(len(train_dataset))]\n",
        "train_embeddings = embed_model.encode(train_questions, convert_to_numpy=True).astype(\"float32\")\n",
        "faiss.normalize_L2(train_embeddings)\n",
        "\n",
        "flat_index = faiss.IndexFlatIP(EMBED_DIM)\n",
        "flat_index.add(train_embeddings)\n",
        "\n",
        "def retrieve_flat(query):\n",
        "    q_emb = embed_normalized(embed_model, query)\n",
        "    _, indices = flat_index.search(q_emb, TOP_K)\n",
        "\n",
        "    context = \"\"\n",
        "    for idx in indices[0]:\n",
        "        item = train_dataset[int(idx)]\n",
        "        context += f\"Example:\\nQ: {item['question']}\\nA: {item['answer']}\\n\\n\"\n",
        "    return context[:1500]\n",
        "\n",
        "def run_flat():\n",
        "\n",
        "    correct = 0\n",
        "    latencies = []\n",
        "    token_usages = []\n",
        "\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "\n",
        "        question = dataset[i][\"question\"]\n",
        "        gt = normalize_answer(extract_answer(dataset[i][\"answer\"]))\n",
        "\n",
        "        context = retrieve_flat(question)\n",
        "        prompt = context + \"\\nNow solve:\\n\" + question\n",
        "\n",
        "        answers = []\n",
        "        sample_lat = []\n",
        "        sample_tok = []\n",
        "\n",
        "        for _ in range(NUM_SAMPLES):\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                    {\"role\":\"user\",\"content\":prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE,\n",
        "                max_tokens=MAX_TOKENS\n",
        "            )\n",
        "\n",
        "            sample_lat.append(time.time()-start)\n",
        "\n",
        "            output = response.choices[0].message.content\n",
        "            pred = normalize_answer(extract_answer(output))\n",
        "\n",
        "            if pred is not None:\n",
        "                answers.append(pred)\n",
        "\n",
        "            if response.usage:\n",
        "                sample_tok.append(response.usage.total_tokens)\n",
        "\n",
        "        if answers:\n",
        "            vote = Counter(answers)\n",
        "            final = vote.most_common(1)[0][0]\n",
        "            if final == gt:\n",
        "                correct += 1\n",
        "\n",
        "        latencies.append(np.mean(sample_lat))\n",
        "        token_usages.append(np.sum(sample_tok))\n",
        "\n",
        "    return correct/len(dataset), np.mean(latencies), np.mean(token_usages)\n",
        "\n",
        "# ==========================================================\n",
        "# IMPROVED HIERARCHICAL MEMORY\n",
        "# ==========================================================\n",
        "\n",
        "class HierarchicalMemory:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.episodic_index = faiss.IndexFlatIP(EMBED_DIM)\n",
        "        self.failure_index = faiss.IndexFlatIP(EMBED_DIM)\n",
        "\n",
        "        self.episodic_store = []\n",
        "        self.failure_store = []\n",
        "\n",
        "    def add_episode(self, question, reasoning, correct, vote_count):\n",
        "\n",
        "        confidence = vote_count / NUM_SAMPLES\n",
        "\n",
        "        emb = embed_normalized(embed_model, question)\n",
        "\n",
        "        if correct and confidence >= 0.7:\n",
        "            self.episodic_index.add(emb)\n",
        "            self.episodic_store.append({\n",
        "                \"question\": question,\n",
        "                \"reasoning\": reasoning\n",
        "            })\n",
        "\n",
        "            if len(self.episodic_store) > MAX_MEMORY_SIZE:\n",
        "                self.episodic_store.pop(0)\n",
        "\n",
        "        elif not correct:\n",
        "            self.failure_index.add(emb)\n",
        "            self.failure_store.append({\n",
        "                \"question\": question,\n",
        "                \"hint\": \"Double-check arithmetic and units.\"\n",
        "            })\n",
        "\n",
        "            if len(self.failure_store) > MAX_MEMORY_SIZE:\n",
        "                self.failure_store.pop(0)\n",
        "\n",
        "    def retrieve(self, question):\n",
        "\n",
        "        context = \"\"\n",
        "\n",
        "        if self.episodic_index.ntotal > 0:\n",
        "            q_emb = embed_normalized(embed_model, question)\n",
        "            _, idx = self.episodic_index.search(\n",
        "                q_emb, min(TOP_K_EPISODIC, self.episodic_index.ntotal)\n",
        "            )\n",
        "            for i in idx[0]:\n",
        "                item = self.episodic_store[int(i)]\n",
        "                context += f\"Similar solved:\\nQ:{item['question']}\\n{item['reasoning']}\\n\\n\"\n",
        "\n",
        "        if self.failure_index.ntotal > 0:\n",
        "            q_emb = embed_normalized(embed_model, question)\n",
        "            _, idx = self.failure_index.search(\n",
        "                q_emb, min(TOP_K_FAILURE, self.failure_index.ntotal)\n",
        "            )\n",
        "            for i in idx[0]:\n",
        "                item = self.failure_store[int(i)]\n",
        "                context += f\"Past mistake warning: {item['hint']}\\n\\n\"\n",
        "\n",
        "        return context[:1500]\n",
        "\n",
        "def run_hierarchical():\n",
        "\n",
        "    memory = HierarchicalMemory()\n",
        "\n",
        "    correct = 0\n",
        "    latencies = []\n",
        "    token_usages = []\n",
        "\n",
        "    for i in tqdm(range(len(dataset))):\n",
        "\n",
        "        question = dataset[i][\"question\"]\n",
        "        gt = normalize_answer(extract_answer(dataset[i][\"answer\"]))\n",
        "\n",
        "        context = memory.retrieve(question)\n",
        "        prompt = context + \"\\nNow solve:\\n\" + question\n",
        "\n",
        "        answers = []\n",
        "        reasoning_samples = []\n",
        "        sample_lat = []\n",
        "        sample_tok = []\n",
        "\n",
        "        for _ in range(NUM_SAMPLES):\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            response = client.chat.completions.create(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
        "                    {\"role\":\"user\",\"content\":prompt}\n",
        "                ],\n",
        "                temperature=TEMPERATURE,\n",
        "                max_tokens=MAX_TOKENS\n",
        "            )\n",
        "\n",
        "            sample_lat.append(time.time()-start)\n",
        "\n",
        "            output = response.choices[0].message.content\n",
        "            reasoning_samples.append(output)\n",
        "\n",
        "            pred = normalize_answer(extract_answer(output))\n",
        "            if pred is not None:\n",
        "                answers.append(pred)\n",
        "\n",
        "            if response.usage:\n",
        "                sample_tok.append(response.usage.total_tokens)\n",
        "\n",
        "        if answers:\n",
        "            vote = Counter(answers)\n",
        "            final, vote_count = vote.most_common(1)[0]\n",
        "            is_correct = (final == gt)\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "        else:\n",
        "            final = None\n",
        "            vote_count = 0\n",
        "            is_correct = False\n",
        "\n",
        "        # store only best reasoning\n",
        "        best_reasoning = reasoning_samples[0] if reasoning_samples else \"\"\n",
        "        memory.add_episode(question, best_reasoning, is_correct, vote_count)\n",
        "\n",
        "        latencies.append(np.mean(sample_lat))\n",
        "        token_usages.append(np.sum(sample_tok))\n",
        "\n",
        "    return correct/len(dataset), np.mean(latencies), np.mean(token_usages)\n",
        "\n",
        "# ==========================================================\n",
        "# RUN ALL SYSTEMS\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n===== BASELINE =====\")\n",
        "baseline = run_baseline()\n",
        "\n",
        "print(\"\\n===== FLAT RETRIEVAL =====\")\n",
        "flat = run_flat()\n",
        "\n",
        "print(\"\\n===== HIERARCHICAL MEMORY =====\")\n",
        "hier = run_hierarchical()\n",
        "\n",
        "# ==========================================================\n",
        "# FINAL COMPARISON\n",
        "# ==========================================================\n",
        "\n",
        "print(\"\\n================ FINAL COMPARISON ================\")\n",
        "print(\"System                Acc     Latency    Tokens\")\n",
        "print(\"--------------------------------------------------\")\n",
        "print(f\"Baseline             {baseline[0]:.4f}   {baseline[1]:.2f}s   {baseline[2]:.1f}\")\n",
        "print(f\"Flat Retrieval       {flat[0]:.4f}   {flat[1]:.2f}s   {flat[2]:.1f}\")\n",
        "print(f\"Hierarchical         {hier[0]:.4f}   {hier[1]:.2f}s   {hier[2]:.1f}\")\n",
        "print(\"===================================================\")\n"
      ]
    }
  ]
}